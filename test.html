<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Face Mesh - TensorFlow.js + MediaPipe</title>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial; display:flex;flex-direction:column;align-items:center;gap:12px;padding:12px }
    #container{position:relative;width:720px;max-width:95vw}
    video, canvas{width:100%;height:auto;border-radius:12px;background:#111}
    canvas{position:absolute;left:0;top:0;pointer-events:none}
    #controls{display:flex;gap:8px}
    button{padding:8px 12px;border-radius:8px;border:1px solid #ddd;background:#fff}
    .hint{font-size:0.9rem;color:#444}
  </style>
</head>
<body>
  <h1>Detección de cara + malla facial (MediaPipe + TensorFlow.js)</h1>
  <div id="container">
    <video id="video" autoplay playsinline></video>
    <canvas id="overlay"></canvas>
  </div>
  <div id="controls">
    <button id="startBtn">Iniciar cámara</button>
    <button id="stopBtn" disabled>Detener</button>
    <label class="hint">FPS: <span id="fps">—</span></label>
  </div>
  <p class="hint">Este ejemplo usa <strong>MediaPipe FaceMesh</strong> para estimar los 468 puntos faciales. También carga TensorFlow.js y BlazeFace (opcional) para mostrar cómo combinarlos si quieres pre-detectar rostros.</p>

  <!-- TensorFlow.js y modelos (carga desde CDN) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface@0.0.7/dist/blazeface.min.js"></script>

  <!-- MediaPipe FaceMesh + helpers desde jsDelivr -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>

  <script>
    // Elementos
    const video = document.getElementById('video');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const fpsLabel = document.getElementById('fps');

    let camera = null;
    let lastTime = performance.now();
    let frameCount = 0;

    // INICIALIZAR FaceMesh de MediaPipe
    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });

    faceMesh.setOptions({
      maxNumFaces: 2,
      refineLandmarks: true, // obtiene puntos extra alrededor de ojos/labios
      minDetectionConfidence: 0.6,
      minTrackingConfidence: 0.5
    });

    faceMesh.onResults(onResults);

    // Opcional: cargar BlazeFace (TensorFlow.js) para pre-detección
    let blazefaceModel = null;
    async function loadBlazeFace(){
      try{
        blazefaceModel = await blazeface.load();
        console.log('BlazeFace cargado');
      }catch(e){
        console.warn('No se pudo cargar BlazeFace (opcional):', e);
      }
    }
    loadBlazeFace();

    function resizeCanvasToVideo(){
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.style.width = video.clientWidth + 'px';
      canvas.style.height = video.clientHeight + 'px';
    }

    async function onResults(results){
      // Calcular FPS simple
      frameCount++;
      const now = performance.now();
      if(now - lastTime >= 1000){ fpsLabel.textContent = frameCount; frameCount = 0; lastTime = now; }

      // Dibujar video en canvas (opcional - lo dejamos transparente para overlay limpio)
      ctx.clearRect(0,0,canvas.width,canvas.height);

      if(results.multiFaceLandmarks && results.multiFaceLandmarks.length){
        for(const landmarks of results.multiFaceLandmarks){
          // Dibujar puntos y conexiones usando drawing_utils de MediaPipe
          drawConnectors(ctx, landmarks, FaceMesh.FACEMESH_TESSELATION, {lineWidth:1});
          drawConnectors(ctx, landmarks, FaceMesh.FACEMESH_RIGHT_EYE, {lineWidth:2});
          drawConnectors(ctx, landmarks, FaceMesh.FACEMESH_LEFT_EYE, {lineWidth:2});
          drawConnectors(ctx, landmarks, FaceMesh.FACEMESH_LIPS, {lineWidth:2});
          for(let i=0;i<landmarks.length;i++){
            const x = landmarks[i].x * canvas.width;
            const y = landmarks[i].y * canvas.height;
            ctx.beginPath();
            ctx.arc(x,y,1.2,0,2*Math.PI);
            ctx.fillStyle = 'rgba(0,200,255,0.9)';
            ctx.fill();
          }
        }
      }
    }

    async function startCamera(){
      // Pide permisos y configura video
      const stream = await navigator.mediaDevices.getUserMedia({ video: { width:1280, height:720 }, audio:false });
      video.srcObject = stream;

      await video.play();
      resizeCanvasToVideo();

      // Usar MediaPipe Camera util para enviar frames a FaceMesh
      camera = new Camera(video, {
        onFrame: async () => {
          // Si quieres usar BlazeFace como pre-check, puedes hacerlo aquí (opcional):
          // if(blazefaceModel){ const predictions = await blazefaceModel.estimateFaces(video, false); /* puedes usar predictions */ }

          await faceMesh.send({image: video});
        },
        width: video.videoWidth,
        height: video.videoHeight
      });
      camera.start();

      startBtn.disabled = true;
      stopBtn.disabled = false;
    }

    function stopCamera(){
      if(camera) camera.stop();
      const tracks = video.srcObject ? video.srcObject.getTracks() : [];
      tracks.forEach(t => t.stop());
      video.srcObject = null;
      startBtn.disabled = false;
      stopBtn.disabled = true;
      ctx.clearRect(0,0,canvas.width,canvas.height);
      fpsLabel.textContent = '—';
    }

    startBtn.addEventListener('click', () => startCamera().catch(err=>{alert('Error accediendo a la cámara: '+err.message)}));
    stopBtn.addEventListener('click', stopCamera);

    // Reajustar canvas cuando cambie el tamaño
    window.addEventListener('resize', () => { if(video.videoWidth) resizeCanvasToVideo(); });

    // Nota: para ejecutar localmente sirve usar un servidor (p. ej. `npx http-server` o la extensión Live Server de VSCode) porque algunos navegadores exigen contexto seguro para getUserMedia.
  </script>
</body>
</html>
